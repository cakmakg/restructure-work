ğŸ¤– RAG-Basierter KI-Chat-Assistent (MERN + AWS S3)
Cloud-Native, skalierbarer und KI-gestÃ¼tzter Dokumenten-Assistent

Dieses Projekt ist eine moderne RAG (Retrieval-Augmented Generation) Anwendung, die es Benutzern ermÃ¶glicht, PDF-Dokumente in die Cloud (AWS S3) hochzuladen und mithilfe von KÃ¼nstlicher Intelligenz (LLM) mit diesen Dokumenten zu chatten.

Im Gegensatz zu herkÃ¶mmlichen Chatbots speichert dieses System die Daten nicht nur als Vektoren, sondern behÃ¤lt die Originaldateien sicher im S3 Object Storage, nutzt fortschrittliche Metadaten-Filterung und liefert dank MongoDB Atlas Vector Search prÃ¤zise Antworten.

ğŸš€ Hauptmerkmale (Key Features)
â˜ï¸ AWS S3 Integration (LocalStack): Hochgeladene Dateien werden nicht auf dem Server-DatentrÃ¤ger, sondern in einer sicheren und skalierbaren Object-Storage-Architektur gespeichert.

ğŸ§  RAG Architektur & Vektorsuche: Dokumente werden in semantische Vektoren umgewandelt. MongoDB Atlas findet mittels "Cosine Similarity" die relevantesten Inhalte.

ğŸ¯ Metadaten-Filterung: WÃ¤hlt der Benutzer eine Kategorie (z.B. "Finanzen"), generiert die KI Antworten ausschlieÃŸlich aus Dokumenten mit diesem Tag (MongoDB Atlas Search Indexing).

âš¡ Streaming Response: Dank der Groq (Llama-3) Integration erscheinen die Antworten ohne VerzÃ¶gerung, Token fÃ¼r Token (Schreibmaschinen-Effekt), auf dem Bildschirm.

ğŸ”’ Sichere Datenverarbeitung: Eine professionelle Pipeline: Datei-Upload -> S3 Backup -> Vektorisierung -> Datenbank-Speicherung.

ğŸ› ï¸ Tech Stack
Dieses Projekt wurde nach Microservices- und Event-Driven-Architekturprinzipien entwickelt:

Backend & Cloud
Node.js & Express: RESTful API Management.

AWS SDK v3: S3 Bucket Management und Dateitransfer.

LocalStack (Docker): Simulation von AWS-Diensten fÃ¼r die lokale Entwicklung.

MongoDB Atlas: Metadaten- und Vektor-Datenbank.

Groq SDK (Llama-3): Ultra-schnelle LLM Engine.

LangChain / Transformers.js: Embedding-Prozesse (Text-zu-Zahl).

Frontend
React (Vite): Hochperformante BenutzeroberflÃ¤che.

Fetch Streams: Auslesen von DatenstrÃ¶men (Ã¤hnlich Server-Side Events).

CSS Modules: Saubere und modulare Strukturierung der Styles.

ğŸ§  Architektur-Flow (Wie es funktioniert)
Das System arbeitet nach dem "Source of Truth" Prinzip:

Upload: Der Benutzer lÃ¤dt ein PDF hoch und wÃ¤hlt eine Kategorie (z.B. "VertrÃ¤ge").

Storage (S3): Die Datei wird im Rohformat in den AWS S3 Bucket hochgeladen und erhÃ¤lt einen eindeutigen s3Key.

Vectorization: Der Inhalt wird gelesen, in kleine StÃ¼cke zerlegt (Chunking) und durch ein Embedding-Modell gejagt.

Indexing (MongoDB): Vektoren + s3Key + Category werden in MongoDB gespeichert.

Retrieval (RAG): Wenn der Benutzer eine Frage stellt:

Wird die Frage in einen Vektor umgewandelt.

Findet MongoDB die Ã¤hnlichsten Vektoren nur innerhalb der gewÃ¤hlten Kategorie.

Generation: Der gefundene Inhalt + die Frage werden an Groq AI gesendet und die Antwort wird generiert.

ğŸ”® Roadmap (ZukunftsplÃ¤ne)
[x] AWS S3 Integration (Erledigt âœ…)

[x] Kategorien-basierte Filterung (Erledigt âœ…)

[ ] Docker Compose fÃ¼r One-Command-Setup.

[ ] Speicherung des Chat-Verlaufs (History) in der Datenbank.

[ ] Vorschau der hochgeladenen Dateien direkt Ã¼ber S3.

ğŸ‘¨â€ğŸ’» Entwickler
GÃ¶khan Cakmak